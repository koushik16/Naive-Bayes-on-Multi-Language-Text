# -*- coding: utf-8 -*-

# Automatically generated by Colaboratory.

# Original file is located at
    # https://colab.research.google.com/drive/1SvpSC56njwali5u_O6eJc1LLAJvUyHP1

# """

import pandas as pd
import numpy as np 
from sklearn.metrics import accuracy_score
from google.colab import drive
drive.mount('/content/drive')

# """**Loading the dataset:**"""

#Arabic
arabic_data = pd.read_csv("/content/drive/MyDrive/programming assignments/Data sets/arabic.txt")

#Greek
greek_data = pd.read_csv("/content/drive/MyDrive/programming assignments/Data sets/greek.txt")

#Japanese
japan_data = pd.read_csv("/content/drive/MyDrive/programming assignments/Data sets/japan.txt")

#American
us_data = pd.read_csv("/content/drive/MyDrive/programming assignments/Data sets/us.txt")

arabic_data.head()

"""**Adding the label column for all the datasets:**"""

arabic_data['Label'] = 'Arabic'

arabic_data.head()

greek_data['Label'] = 'Greek'

greek_data.head()

japan_data['Label'] = 'Japanese'

japan_data.head()

us_data['Label'] = 'American'

us_data.head()
arabic_data.shape
greek_data.shape
japan_data.shape
us_data.shape

"""**Combining all the four datasets:**"""

merge_data = pd.concat([arabic_data,greek_data,japan_data,us_data])

merge_data.shape

"""**Data Preprocessing:**"""

from sklearn.feature_extraction.text import CountVectorizer

# Using CountVectorizer to transform the given names into a vector 

vectorizer = CountVectorizer().fit(merge_data['Names'])
Input = vectorizer.transform(merge_data['Names'])
X_input = Input.toarray()

from sklearn.preprocessing import LabelEncoder

#converting the labels into a numeric form (to unique integers)

label_encoder = LabelEncoder().fit(merge_data['Label'])
label = label_encoder.transform(merge_data['Label'])

"""**Train-test split:**"""

#splitting the dataset into train and test sets with shuffle = true

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_input, label, test_size=0.3, shuffle = True, random_state=42)

X_train.shape

X_test.shape

"""**Model Implementation:**"""

#Multinomial naive bayes from scratch
class MultinomialNaiveBayes:

    def __init__(self,lap_smooth=1):
        self.lap_smooth = lap_smooth
    
    #For each unique class in y, calculating prior 
    def prior(self): 
        prior_y = np.zeros((self.n_labels))
        _, self.unique = np.unique(self.y,return_counts=True)
        for i in range(self.labels.shape[0]):
            prior_y[i] = self.unique[i] / self.n_sample_names
        return prior_y
            
    #Fitting the labels according to the sample names
    def fit(self, X, y): 
        self.y = y
        self.n_sample_names, self.n_feat = X.shape
        self.labels = np.unique(y)
        self.n_labels = self.labels.shape[0]
        #list of priors for each y
        self.label_priors = self.prior() 
            
        self.c_v = np.zeros((self.n_labels, self.n_feat))
        self.c_v_bar = np.zeros((self.n_labels)) 
        for i in self.labels: 
            index = np.argwhere(self.y==i).flatten()
            columns_sum = []
            for j in range(self.n_feat):
                columns_sum.append(np.sum(X[index,j]))
                
            self.c_v[i] = columns_sum 
            self.c_v_bar[i] = np.sum(columns_sum) 
    
    #Calculates likelihood, finds the best probability for a sample name given a particular label
    def likelihood(self, x, k):
        ind_likelihood = []
        i = 0
        while i < x.shape[0]:
            # c_v is the count of features with a feature value of x_i for feature index i and class label k.
            # c_v_bar is the number of features in class k.
            c_v = self.c_v[k,i]
            c_v_bar  = self.c_v_bar[k]
            temp =  ((c_v + self.lap_smooth )/ (c_v_bar + (self.lap_smooth * self.n_feat)))**x[i]
            ind_likelihood.append(temp)
            i += 1
        
        return np.prod(ind_likelihood)
    
    #Predicting the label of the test data
    def predict(self, X):
        sample_names, feat = X.shape
        self.prob_predict = np.zeros((sample_names,self.n_labels))
        combined_likelihood = np.zeros((self.n_labels))
        i = 0
        while i < X.shape[0]:
            total_likelihood = 0
            k = 0
            while k < self.n_labels:
              combined_likelihood[k]  = self.label_priors[k] * self.likelihood(X[i],k) # P(y) P(X|y) 
              total_likelihood += combined_likelihood[k]
              k+=1
            k = 0
            while k < self.n_labels:
              self.prob_predict[i,k] = (combined_likelihood[k] / total_likelihood)
              k+=1
            i += 1
            
        index = np.argmax(self.prob_predict,axis=1)
        return self.labels[index]

"""**Model evaluation:**"""

Bayes_classifier = MultinomialNaiveBayes()
Bayes_classifier.fit(X_train, y_train)
yhat = Bayes_classifier.predict(X_test)
print(accuracy_score(y_test,yhat))

"""**Comparing the above results with built-in model:**"""

#importing the built-in Naive Bayes classifier to compare the accuracy 

from sklearn.naive_bayes import MultinomialNB

mnb = MultinomialNB()
mnb.fit(X_train, y_train)

y_predict = mnb.predict(X_test)

y_predict

from sklearn.metrics import accuracy_score

print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_predict)))

